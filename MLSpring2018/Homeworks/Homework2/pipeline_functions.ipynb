{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for the pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.tree as tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import graphviz \n",
    "%matplotlib inline\n",
    "import pylab as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 READ THE DATA\n",
    "\n",
    "def create_df (file):\n",
    "    '''Creates a pandas data frame using as input \n",
    "    a .csv file\n",
    "    Inputs:\n",
    "    file (string): Path to file\n",
    "    Returns:\n",
    "    Dataframe\n",
    "    '''\n",
    "    return pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(df, var_y,var_x):\n",
    "    '''\n",
    "    Creates a scatterplot using a dataframe df,\n",
    "    and the features var_y and var_x\n",
    "    Inputs:\n",
    "    df-> Dataframe\n",
    "    var_y (string)-> y variable\n",
    "    var_x (string) -> x variable\n",
    "    '''\n",
    "    plt.scatter(df[var_y], df[var_x])\n",
    "    plt.title(var_y +' '+ 'vs'+ ' '+ var_x)\n",
    "    plt.xlabel(var_x) \n",
    "    plt.ylabel(var_y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df, variable, num_bins):\n",
    "    '''\n",
    "    Creates a histogram plot using a dataframe df for\n",
    "    a variable of interest\n",
    "    \n",
    "    Inputs:\n",
    "    df-> Dataframe\n",
    "    variable -> variable of interest\n",
    "    num_bins -> number of bins\n",
    "    '''\n",
    "    plt.hist(df[variable].dropna(), bins=num_bins)\n",
    "    plt.title('Distribution of'+ ' '+  variable)\n",
    "    plt.ylabel(variable)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_pair(df, variable):\n",
    "    '''\n",
    "    Creates a dataframe with the correlation values of\n",
    "    the variable and the rest of the variables in a dataframe\n",
    "    \n",
    "    Inputs:\n",
    "    df-> dataframe\n",
    "    variable: the variable of interest which the correlation coefficients\n",
    "    are calculated with\n",
    "    \n",
    "    Returns: \n",
    "    Dataframe\n",
    "    '''\n",
    "    dic = {}\n",
    "    for element in list(set(list(df)) - set(list(variable))):\n",
    "        dic[element] = df[variable].corr(data[element])\n",
    "    a = pd.DataFrame(list(dic.items()), columns=['variable', variable])\n",
    "    a.set_index('variable', inplace= True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nulls(df):\n",
    "    '''\n",
    "    Counts the number of missing values per variable in a dataframe\n",
    "    Returns: Dataframe\n",
    "    '''\n",
    "    dic = {}\n",
    "    for element in list(df):\n",
    "        dic[element] = sum(pd.isnull(data[element]))\n",
    "    a = pd.DataFrame(list(dic.items()), columns=['variable', 'Missings'])\n",
    "    a.set_index('variable', inplace= True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from DataGotham\n",
    "#To treat outliers\n",
    "def cap_values(x, cap):\n",
    "    '''\n",
    "    if a value exceeds the threshold cap, it returns\n",
    "    the value cap, otherwise the same value'''\n",
    "    if x > cap:\n",
    "        return cap\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-299fb534674e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Imputing nulls. Modify this code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Imputing nulls. Modify this code\n",
    "df=df\n",
    "is_test = np.random.uniform(0, 1, len(df)) > 0.75\n",
    "train = data[is_test==False]\n",
    "test = data[is_test==True]\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "income_imputer = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "#split our data into 2 groups; data containing nulls and data \n",
    "# not containing nulls we'll train on the latter and make\n",
    "# 'predictions' on the null data to impute monthly_income\n",
    "train_w_monthly_income = train[train.MonthlyIncome.isnull()==False]\n",
    "train_w_null_monthly_income = train[train.MonthlyIncome.isnull()==True]\n",
    "cols = ['col1', 'col2']\n",
    "income_imputer.fit(train_w_monthly_income[cols], train_w_monthly_income.MonthlyIncome)\n",
    "new_values = income_imputer.predict(train_w_null_monthly_income[cols])\n",
    "train_w_null_monthly_income['monthly_income'] = new_values\n",
    "#combine the data back together\n",
    "train = train_w_monthly_income.append(train_w_null_monthly_income)\n",
    "train.head()\n",
    "\n",
    "# I NEED TO REVIEW HOW TO IMPLEMENT THE FINAL IMPUTATION HERE\n",
    "data_with_null = data[data.MonthlyIncome.isnull() == True]\n",
    "data_no_null = data[data.MonthlyIncome.isnull() == False]\n",
    "cols = ['NumberRealEstateLoansOrLines', 'NumberOfOpenCreditLinesAndLoans']\n",
    "new_values = income_imputer.predict(data_with_null[cols])\n",
    "data_with_null['monthly_income'] = new_values\n",
    "\n",
    "dfs = [data_no_null , data_with_null]\n",
    "data_imputed = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_mean(df, variable):\n",
    "    '''Replaces missing values with the\n",
    "    mean value of the variable\n",
    "    Inputs:\n",
    "    df-> Dataframe\n",
    "    variable-> variable where NaN are to be replaced\n",
    "    Returns:\n",
    "    Panda Series\n",
    "    '''\n",
    "   \n",
    "    return df[variable].fillna(df[variable].mean(), inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(x):\n",
    "    '''Assigns the median value of an upward outlier\n",
    "    Inputs: Dataframe\n",
    "    Returns: Dataframe\n",
    "    \n",
    "    i.e. data['DebtRatio_wo_ol'] = cap_outliers(data['DebtRatio'])\n",
    "    '''\n",
    "       \n",
    "    mask = (x >= x.quantile(.95))\n",
    "    x[mask] = x.median()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df, variable, statistic):\n",
    "    '''Imputes values to a specified variable\n",
    "    Inputs:\n",
    "        df: dataframe\n",
    "        variable: variable which will be imputed\n",
    "        statistic: valid values: mean, median or zero\n",
    "    Returns:\n",
    "        dataframe without nulls\n",
    "    '''\n",
    "    if statistic == 'mean':\n",
    "        df[variable].fillna(df[variable].mean(), inplace=True)\n",
    "    elif statistic == 'median':\n",
    "        df[variable].fillna(df[variable].median(), inplace=True)  \n",
    "    elif statistic == 0:\n",
    "        df[variable] = np.where(df[variable].isnull(), statistic,\n",
    "                                  df[variable])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Generate and Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_buckets(df, variable, buckets):\n",
    "    '''\n",
    "    Creates a df which with the variable cut into buckets\n",
    "    Inputs:\n",
    "        df-> Dataframe where the new variable is added\n",
    "        variable -> variable to be cut into bukets\n",
    "        buckets-> List of values in which the variable will be cut into\n",
    "    Returns:\n",
    "        Dataframe\n",
    "    '''\n",
    "    df = pd.cut(df[variable], buckets, labels = (list(range(len(buckets))))[1:])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_in_pct_bins(df, variable, quantiles):\n",
    "    '''\n",
    "    Creates bins for the variable and dataframe specified\n",
    "    Inputs:\n",
    "        df -> Data Frame\n",
    "        variable (String)-> Variable of interest\n",
    "        quantiles (list) -> List of cutoffs for quantiles\n",
    "    Returns:\n",
    "        Series Dataframe\n",
    "    '''\n",
    "    bins = []\n",
    "    for q in quantiles:\n",
    "        bins.append(df[variable].quantile(q))\n",
    "    labels = []\n",
    "    for element in (list(range(len(quantiles))))[1:]:\n",
    "        label = element\n",
    "        labels.append(label)\n",
    "        \n",
    "    return pd.cut(df[variable], bins=bins, include_lowest = True, labels = labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy(df, variable, threshold, relation):\n",
    "    ''' \n",
    "    Creates a series dataframe with dummy variables equals to one\n",
    "    if the variable is equal, less or greater than a threshold.\n",
    "    Inputs:\n",
    "        df: dataframe\n",
    "        variable (string): variable to create the dummy variable\n",
    "        threshold (int): value that makes the relation true in order to create the dummy\n",
    "        relation (string): can take three values: 'greater', 'equal', or 'less'.\n",
    "    Returns:\n",
    "        Series Dataframe\n",
    "    '''   \n",
    "    if relation == 'greater':\n",
    "        return df[variable].apply(lambda x: 1 if x > threshold else 0)\n",
    "    elif relation == 'equal':\n",
    "        return df[variable].apply(lambda x: 1 if x == threshold else 0)\n",
    "    elif relation == 'less':\n",
    "        return df[variable].apply(lambda x: 1 if x < threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best_features(df,features, dep_var):\n",
    "    '''\n",
    "    Creates a barplot to show the better predictors\n",
    "    Inputs:\n",
    "        df: dataframe\n",
    "        feautures (list): features that will be evaluated\n",
    "        dep_var (string): variable of interest\n",
    "    '''\n",
    "    features = np.array(features)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(df[features], df[dep_var])\n",
    "    importances = clf.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)\n",
    "    padding = np.arange(len(features)) + 0.5\n",
    "    pl.barh(padding, importances[sorted_idx], align='center')\n",
    "    pl.yticks(padding, features[sorted_idx])\n",
    "    pl.xlabel(\"Relative Importance\")\n",
    "    pl.title(\"Variable Importance\")\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Build and Evaluating Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_and_test(df, predictors, var_to_predict, test_size, random_state):\n",
    "    '''Creates train and test dataframes to run a model\n",
    "    Inputs:\n",
    "        df: Dataframe\n",
    "        predictors(list): list of predictor variables(string)\n",
    "        test size (float): share of the data to construct the training and test dfs\n",
    "    Returns:\n",
    "        Tuple with 'x_train', 'x_test', 'y_train', and 'y_test'\n",
    "        '''\n",
    "    X = df.filter(items = predictors)\n",
    "    Y = df[var_to_predict]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state = random_state)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tree(x_train, x_test, y_train, y_test, depths, criterion, \n",
    "                  max_features, splitter, min_sample_l, min_sample_spl, max_leaf_n):\n",
    "    '''\n",
    "    Creates a dataframe with the different models' accuracy predictions, and returns\n",
    "    a dataframe with all these models, sorted by degree of accuracy in the test data\n",
    "    Inputs:\n",
    "        x_train: dataframe with the features that predict the outcome of interest. Used for \n",
    "            training the model\n",
    "        x_test: dataframe with the features that predict the outcome of interest. Used for testing\n",
    "            the model\n",
    "        y_train: variable to predict. Used to train the model\n",
    "        y_test: variable to predict. Used to test the model\n",
    "        depths (list of integers): maximum depth of the tree\n",
    "        criterion (list): Measure of purity of subgroups. 'Entropy' or 'Gini'\n",
    "        max_features (list): maximum number of features to include in the model\n",
    "        splitter (list): Method to split samples. 'Best' or 'Random'\n",
    "        min_sample_l(list): min number of samples required to be at a leaf node.\n",
    "        min_samples_split: minimum number of samples required to split an internal node \n",
    "        max_leaf_nodes (list): max number of leaf nodes a tree can have\n",
    "    '''\n",
    "    for d in depths:\n",
    "        for criteria in criterion:\n",
    "            for feature in max_features:\n",
    "                for split in splitter:\n",
    "                    for min_leave in min_sample_l:\n",
    "                        for min_split in min_sample_spl:\n",
    "                            for max_leaf in max_leaf_n:\n",
    "                                dec_tree = DecisionTreeClassifier(max_depth= d, criterion = criteria, splitter = split,\n",
    "                                max_features = feature, min_samples_leaf = min_leave,  min_samples_split = min_split )\n",
    "                                dec_tree.fit(x_train, y_train)\n",
    "                                train_pred = dec_tree.predict(x_train)\n",
    "                                test_pred = dec_tree.predict(x_test)\n",
    "                                # evaluate accuracy\n",
    "                                train_acc = accuracy(train_pred, y_train)\n",
    "                                test_acc = accuracy(test_pred, y_test)\n",
    "                                result = d, criteria, feature, split, min_leave, min_split, max_leaf, train_acc, test_acc\n",
    "                                results.append(result)        \n",
    "    evaluation_df = pd.DataFrame(results,columns = ['Max_depth', 'Criteria', 'Number_Features', 'Split', \\\n",
    "                                                'Min_Sample_leave','Min_Sample_split','Max_leaf_nodes', 'Train_Acc', 'Test_Acc'])\n",
    "    return evaluation_df.sort_values(by=['Test_Acc'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATION USING CONFUSION MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION TO VISUALIZE THE MODEL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
